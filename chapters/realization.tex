\chapter{Realisatie}
\label{ch:realization}

\section{Software}

De software van het Autonerf systeem is geïmplementeerd in C++ waarbij gebruikt
wordt gemaakt van de libraries van OpenCV. OpenCV is een groep libraries die
ontwikkeld zijn voor \emph{computer vision} (OpenCV staat dan ook voor
\emph{Open Computer Vision}).

De software is \emph{event driven} geschreven. Dit betekend dat, in plaats dat
er gepolled wordt door object voor data, er met events wordt gewerkt. Wanneer
er binnen een object een event plaatsvindt, worden andere \emph{subscribers}
hier van op de hoogte gebracht.

Events worden met een basisklasse (\code{Autonerf::Emitter}) ge-\emph{emit}.
Objecten kunnen door een handler te registreren (met behulp van de \code{on}
methode) ``luisteren'' voor events.

\subsection{De \emph{reader}}

Voor het uitlezen van de camera beelden door de \code{Reader} klasse wordt
gebruik gemaakt van de OpenCV klasse \code{cv::VideoCapture}. Deze klasse geeft
een \code{cv::Mat} object wat een RGB frame representeerd (zie \ref{ls:reader}).

\begin{listing}[H]
    \begin{cppcode}
    cv::Mat frame;

    if (!this->capture.isOpened()) {
        throw std::runtime_error("Cannot capture frame from closed device.");
    }

    if (!this->capture.grab()) {
        return;
    }

    if (!this->capture.read(frame)) {
        throw std::runtime_error("Could not read frame from capture device.");
    }

    this->emit("frame", json_integer((json_int_t) &frame));
    \end{cppcode}
    \caption{Het uitlezen van een frame van de camera.}
    \label{ls:reader}
\end{listing}

\vfill
\pagebreak

\subsection{De \emph{detector}}

Alle subsystemen van het \emph{detector} subsysteem worden geïmplementeerd door
de OpenCV klasse \code{cv::CascadeClassifier}. Deze klasse maakt gebruik van
Haar cascades om gezichten te detecteren in beelden (zie listing \ref{ls:classifier}).

\begin{listing}[H]
    \begin{cppcode}
    std::vector<cv::Rect> faces;

    this->classifier.detectMultiScale(
        grayscale,
        faces,
        1.1,
        3,
        CV_HAAR_FIND_BIGGEST_OBJECT|CV_HAAR_SCALE_IMAGE,
        cv::Size(50, 50)
    );

    if (face.size() > 0) {
        this->emit(
            "detected",
            // informatie van locatie grootste gezicht in beeld
        );
    }
    \end{cppcode}
    \caption{Het detecteren van gezichten met de \code{cv::CascadeClassifier} klasse}
    \label{ls:classifier}
\end{listing}

\subsubsection{Enhancer}

Het frame wat uitgelezen is door de \emph{reader} klasse wordt geconverteerd
naar een grayscale beeld met behulp van de \code{cv::cvtColor} (zie listing \ref{ls:det-enhc})
method. Dit wordt gedaan omdat de \emph{Segmentator} een grayscale beeld verwacht.
Daarnaast wordt er een \emph{histogram equalization} uitgevoerd. Hiermee wordt
het contrast van het beeld verbeterd waardoor de intensiteit beter verdeeld is.
Door dit te doen worden details in het hele beeld beter zichtbaar, waardoor de
de gezichtsdetectie en -herkenning makkelijker verlopen. Figuur \ref{fig:sinterklaas1}
en \ref{fig:sinterklaas2} laat een voorbeeld zien van deze operatie.

\begin{listing}[H]
    \begin{cppcode}
    cv::cvtColor(frame, grayscale, CV_BGR2GRAY);

    cv::equalizeHist(grayscale, grayscale);
    \end{cppcode}
    \caption{Het \emph{enhancen} van uitgelezen camera beelden}
    \label{ls:det-enhc}
\end{listing}

\begin{figure}[H]
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\linewidth]{figures/sint-before.png}
        \caption{Voor \emph{histogram equalization}}
        \label{fig:sinterklaas1}
    \end{minipage}
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\linewidth]{figures/sint-after.png}
        \caption{Na \emph{histogram equalization}}
        \label{fig:sinterklaas2}
    \end{minipage}
\end{figure}

\subsubsection{Segmentator}

De \emph{segmentator} verzorgt de segmentatie van ontvangen frames. Het hele frame
afgegaan waarbij elke pixel wordt vergeleken met zijn omgeving. De pixel waarden
van de omgeving worden geïntegreerd waarna deze waarde in de pixel wordt gezet
die met zijn omgeving is vergeleken.

\subsubsection{Feature extractor}

Nadat alle pixels zijn geïntegreerd met hun omgeving wordt er gekeken welke Haar
features er aanwezig zijn in het beeld. Dit wordt gedaan door van elke Haar
feature de gemiddelde pixelwaarde van het donkere deel af te trekken van de
gemiddelde pixel waarde van het lichte deel. Als deze waarde boven een bepaalde
threshold ligt wordt er aangenomen dat de Haar feature aanwezig is in de regio
van het beeld waar op dat moment naar wordt gekeken. OpenCV verzorgd standaard
een aantal files die Haar features beschrijven, waaronder één die een gezicht
van de voorkant beschrijft. In het Autonerf systeem wordt dit bestand gebruikt.

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.75]{figures/haar-features.png}
        \caption{Haar features in een gezicht}
    \end{center}
\end{figure}

\vfill
\pagebreak

\subsubsection{Classifier}

De door de \emph{feature extractor} beschreven Haar features en thresholds worden
bepaald door een techniek voor \emph{machine-learning} genaamd AdaBoost. Met deze
techniek wordt er per regio gekeken of een door AdaBoost geselecteerde Haar
feature aanwezig is. Als dit niet het geval is wordt de regio direct afgeschreven
als gezicht. Als het wél het geval is gaat de regio door naar de volgende \emph{stage}
en wordt een andere feature geselecteerd, et cetera. Als de regio succesvol alle
\emph{stages} passeert is er sprake van een gezicht.

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.75]{figures/adaboost-classification.png}
        \caption{AdaBoost classificatie}
    \end{center}
\end{figure}

\subsection{De \emph{localizer}}

Het lokaliseren van gezichten is eigenlijk een triviale taak. Zodra de coordinaten
(binnen het frame) bekend zijn van het gezicht, kan hiermee de ruimtelijke
afwijking van het gezicht ten opzichte van het centrum van het frame.

Als we het centrum van het frame $(0, 0)$ dopen, kan de locatie van het gezicht
worden geschreven als $(x, y)$. Een camera heeft een zogenaamd
\emph{field of view} (FOV) wat in graden wordt uitegedrukt. Door middel van deze
eigenschap van de camera, kan precies berekend worden hoeveel graden per pixel
er in het frame zitten (zowel horizontaal als verticaal).

Als voorbeeld nemen we een camera die een resolutie heeft van $640 \times 480$
pixels en een FOV van $45\,^{\circ}$. Hieruit volgt dat het aantal graden per pixel
is: $45\,^{\circ} / 640 = 0.07{\rm \,^{\circ}/p}$ horizontaal en
$45\,^{\circ} / 480 = 0.09{\rm \,^{\circ}/p}$ verticaal. De algemene vergelijking
is te zien in \ref{eq:dpp}. In deze vergelijking is $G_x,_y$ het aantal graden
per pixel in het frame. $F$ is de \emph{field of view} van de camera en $R_x,_y$
is de resolutie van de $x$ en $y$ as.

\begin{equation}
    G_x,_y = \frac{F}{R_x,_y}
    \label{eq:dpp}
\end{equation}

\vfill
\pagebreak

Daarnaast moet er ook een afwijking in pixels worden berekend. Om dit te doen
wordt gebruik gemaakt van het originele frame dat ontvangen is. Door de resolutie
van dit frame te gebruiken (de $R,_y$ uit de vergelijking \ref{eq:dpp}) kan de
afwijking zowel horizontaal als vertikaal worden berekend (vergelijking \ref{eq:offset}).

\begin{equation}
    \Delta_x,_y = {L_x,_y} - {C_x,_y}
    \label{eq:offset}
\end{equation}

In deze vergelijking is $\Delta_x,_y$ de \emph{offset}, $C_x,_y$ is het centrum
van het frame (als een frame $(640,480)$ pixels is, is het
centrum van dit frame $(320,240)$). $L_x,_y$ is de locatie van het gedetecteerde
gezicht wat door de \emph{detector} gedetecteerd is.

Door vergelijking \ref{eq:dpp} en \ref{eq:offset} combineren, is het mogelijk
de offset van het gedetecteerde gezicht in graden uit te drukken (vergelijking
\ref{eq:combined}).

\begin{equation}
    \Delta_x,_y = \frac{F}{R_x,_y}({L_x,_y} - {C_x,_y})
    \label{eq:combined}
\end{equation}

Het is echter voor het systeem niet mogelijk om precies op 0 te komen, daarom
is er een foutmarge geïmplementeerd (de \emph{margin}, $m$). Als $\Delta_x,_y$ binnen
deze \emph{margin} valt, wordt de offset naar 0 gebracht.

\begin{equation}
    \Delta_x,_y = \begin{cases}
        \Delta_x,_y, & \text{als } \Delta_x,_y \geq m\\
        0 & \text{als } \Delta_x,_y < m
    \end{cases}
\end{equation}

Zoals te zien in listing \ref{ls:offset} is dit relatief makkelijk te implementeren
in C++.

\begin{listing}[H]
    \begin{cppcode}
    double dpp[] = {
        ((double) this->fov) / ((double) frame.cols)),
        ((double) this->fov) / ((double) frame.rows))
    };
    int offset[] = {
        (x - (frame.cols / 2)),
        (y - (frame.rows / 2))
    };
    int radial[] = {
        ((double) offset[0]) * dpp[0],
        ((double) offset[1]) * dpp[1]
    };

    if (offset[0] < this->margin &&
        offset[1] < this->margin &&
        offset[0] > -(this->margin) &&
        offset[1] > -(this->margin)) {
        this->emit(
            "fire"
        );
    } else {
        this->emit(
            "localized",
            // Location data
        );
    }
    \end{cppcode}
    \caption{Het \emph{localizen} van gedetecteerde gezichten}
    \label{ls:offset}
\end{listing}

\vfill
\pagebreak

\subsection{De \emph{controller}}

De \emph{controller} is verantwoordelijk voor het aansturen van het Autonerf
systeem. Dit gebeurd middels een seriële verbinding waar JSON-data over verstuurd
wordt. Voor de seriële communicatie wordt gebruik gemaakt van een Boost library
(\code{boost::asio}).

Omdat alle data binnen het systeem ook als JSON wordt verzonden, kunnen de
events van die de \emph{localizer} klasse verstuurd gebruikt worden om het
embedded systeem aan te sturen. Om deze rede heeft de \emph{controller} weinig
logica. Er wordt alleen geschreven naar een seriele port (bijvoorbeeld \code{/dev/tty.usbmodem}).
Hoe dit wordt gedaan wordt getoond in listing \ref{ls:serial}. In deze listing
is de \code{this->serial} variabele een object van de \code{boost::asio::serial\_port}
klasse. De data die verzonden wordt is te zien in listing \ref{ls:output}.

\begin{listing}[H]
    \begin{cppcode}
    void
    Controller::write(const std::string& data)
    {
        boost::asio::write(this->serial, boost::asio::buffer(data.c_str(), data.size()));
    }
    \end{cppcode}
    \caption{Schrijven naar een seriële port}
    \label{ls:serial}
\end{listing}

\begin{listing}[H]
    \begin{jsoncode}
    {"command": "aim", "options": {"offset": [-0.38671875, 3.4375]}}
    {"command": "fire", "options": {}}
    \end{jsoncode}
    \caption{Output over de seriële port}
    \label{ls:output}
\end{listing}

\section{Hardware}

Het lanceer systeem met camera wordt op een platform van hout (lasergesneder)
gemonteerd. Dit betekend dat er 10 lucht slangen, een camera kabel en tilt servo
kabel als bundel naar het platform worden geleid. De lucht kleppen zijn op het
lanceer platform geplaatst, waardoor de lucht slangen een geringe lengte hebben.

\subsection{Shot control}

De shot control is een array van 10 MOSFETs die minimaal 1A kunnen schakelen
ter behoeve van de inschakelstroom van de kleppen. Daarnaast dient er een
blusdiode bij of in te zitten om de uitschakel piek te dempen. Een extern
signaal zorgt er voor dat de kleppen een bepaalde tijd open blijven of kan zorgen
zelfs voor een Rapid Boost™ lancering.

\subsection{Tilt control}

Tilt control wordt gestuurd door een servo motor die met een arm aan het lanceer
compartiment wordt bevestigd. De servo zal verder worden bestuurd via een
externe constant PWM source die zorgt voor de juiste frequentie en timing van
het pulsen signaal.

\subsection{Pan control}

Pan control wordt ook gestuurd via een servo. Het pan platform wordt direct op
de servo gemonteerd.

\subsection{CPU}

De I/O controller is voorzien van een CPU om seriële commando’s vanuit de PC te
kunnen verwerken. De servo’s hebben een constant signaal nodig, dit is een van
de taken van de CPU. Daarnaast dienen er schoten gelost te kunnen worden, de
CPU bepaald hoe lang een luchtklep open staat. Zodoende kan het nooit zo zijn
dat er een klep overbodig lang open blijft staan.

Als CPU hebben we een Arduino gebruikt. Dit omdat een Arduino goedkoop is en
makkelijk 2 PWM kanalen kan aansturen. De kleppen worden aangestuurd via GPIO
pinnen, er zitten voldoende pinnen op de Arduino om dit te aan te kunnen sturen.
Ook seriële communicatie kan doordat er reeds en USB naar UART conversie IC
opzit ten behoeven.

\subsection{Veiligheid}

Om het onverwachts schieten te voorkomen wordt er een uitgebreide beveiliging
ingebouwd (zie bijlagen \ref{app:schematics} en \ref{app:pcb}). Deze beveiliging
zal voornamelijk in de hardware terug te vinden zijn, omdat dit de meest
betrouwbare bron is voor beveiliging.

De lucht kleppen kunnen niet geactiveerd worden wanneer de \code{Reset} actief
is. Om daar een extra veiligheid aan toe te voegen moet het veiligheidscircuit
langer actief blijven dan de microcontroller zodat er ook niks tijdens het booten van de
software kan gebeuren. Daarnaast moet hij ook nog geactiveerd worden door een
losse pin op de microcontroller. En tot slot zit er een extra \code{Enable shot} schakelaar
op die via een flip-flop wordt geactiveerd (push button). Wanneer er spanning
weg valt, een reset wordt uitgevoerd of iets anders zonder dat deze schakelaar
uit is gezet zal hij gereset worden. Zo wordt er gezorgd voor een maximale
beveiliging als het systeem aan staat en de microcontroller wordt geprogrammeerd.

\subsection{Behuizing}

De behuizing van het systeem wordt een 10-stage raket lanceer station. Iedere
stage krijgt dus 1 buisje ter behoeve van de luchtdruk. Er kunnen dus maximaal 10
schoten worden gelost voor er herladen dient te worden.

\subsubsection{Launcer}

De zo genaamde \emph{launcher} is het beweegbare compartiment waar alle
Nerf darts in worden geladen om af te schieten (zie bijlage \ref{app:launcher}).
Deze richt ook op de persoon.

De servo voor het tilt systeem wordt op het pan platform gemonteerd en met een
arm aan de launcher bevestigd. Dit wordt gedaan om de last op de servo te
verminderen en de respons van het systeem dus beter te maken. Ook wordt er
aanbevolen om het scharnierpunt niet helemaal achteraan te leggen zodat er een
contragewicht is om nog meer last van de servo te halen.

Om het juiste montage punt van de arm van de servo te bepalen is er een serie
driehoek berekeningen nodig. Daarnaast kan er middels deze berekeningen ook de
hoogte worden bepaald ten opzichte van het pan platform (in verband met de
neerwaartse beweging van het contragewicht).

Figuur \ref{fig:servo-montage} toont een schematische weergave van het systeem.
In deze figuur is $S_1$ de lengte van het scharnierpunt tot de voorkant, $S_2$
is de lengte van het scharnierpunt tot de achterkant. $\alpha$ geeft de maximale
hoek aan van de \emph{launcher} en $\Delta{B}$ is de uitslag van de achterkant (
de minimale hoogte van de \emph{launcher}).

Het bevestigingspunt van de arm van de servo op lengte $S_1$ ten opzichte van het
kantelpunt kan worden bepaald met de formule \ref{eq:bevestiging}.

\begin{equation}
    P = \frac{2r}{\text{sin } \alpha}
    \label{eq:bevestiging}
\end{equation}

In deze formule is $r$ de lengte van de as tot het bevestigingspunt van de arm
van de servo. Bij een kruisvormige servo-arm zal deze afstand ongeveer 16 \unit{mm}
zijn. $P$ is het bevestiginspunt op lengte $S_1$.

\begin{figure}
    \includegraphics{figures/servo-montage.png}
    \caption{Schematische weergave servo montage punt}
    \label{fig:servo-montage}
\end{figure}

Door gebruik te maken van formule \ref{eq:bevestiging} is het mogelijk om de minimale
montage hoogte van de launcher te bepalen:

\begin{equation}
    \Delta{B} = S_2 \text{sin } a
\end{equation}

Bij het maken van deze berekening is het belangrijk rekening te houden met het
bevestigingspunt $P$ zodat de servo niet te veel kracht moet zetten (dus niet
te dicht bij het scharnierpunt). Daarnaast is het belangrijk wel een goede
hoek/actieradius te hebben. Dit is afhankelijk van de kijkhoek van de
camera en op welke afstand het systeem mensen moet kunnen herkennen/raken.

\subsubsection{Camera's}

De camera’s komen boven op het lanceer station zodat de software een gezicht
kan detecteren en de motor vervolgens naar een bepaald vast punt in het camera
beeld dient te bewegen. Voor de veiligheid kan er besloten worden dit punt wat
hoger te leggen als het midden van het scherm. Zo bestaat er een kleinere kans
dat iemand daadwerkelijk in het gezicht geraakt wordt.

\subsubsection{Pan platform}

Het pan platform (zie bijlage \ref{app:pan-platform} is enkel voorzien van een
montage houder voor de tilt servo en voetjes voor de montage van het lanceer
platform.

Bij het ontwerp van dit platform moet rekening gehouden worden met het
zwaartepunt. Dit moet zo ver mogelijk in het midden van de pan servo liggen om
een stabiel systeem te hebben. Omdat het zwaarte punt constant veranderd door
het aantal Nerf darts en tilt hoek nemen we het midden van de ‘launcher’ als
zwaartepunt. Dan zal de tilt servo ongeveer op het tilt punt komen te zitten.

\subsubsection{Basisstation}

Het laatste onderdeel is het platform waarin de pan servo wordt gemonteerd en de
aansturing. Het basisstation (zie bijlage \ref{app:base-platform}) moet een
goede constructie voor de pan servo geven zodat deze stabiel kan bewegen.
Daarnaast wordt op de achterkant de aansturing bevestigd. Het is belangrijk dat
de nodige beveiligingssystemen die op de aansturing zitten eenvoudig te bedienen
zijn. De aansturing wordt dan ook bij voorkeur achter het lanceer systeem en
buiten het bereik van het pan platform.
